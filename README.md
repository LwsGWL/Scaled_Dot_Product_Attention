# Precautions before starting
  I'm a first-year high school student (at the time of writing this project) who dreams of becoming an AI developer. 
  This project was created for personal learning purposes and other purposes, and this is my first time uploading it to GitHub. 
  Therefore, there may be errors, typos, or inaccuracies in various parts of the project, including references and code. 
  Thank you for your understanding. 
  (Feedback is always welcome!)
  
# Scaled_Dot_Product_Attention
This is the Scaled_Dot_Product_Attetion layer, implemented based on research from the "Attention is All You Need" paper and several open-source converters on GitHub. For detailed sources, please refer to the READM.md file.

## Reference
  - "Attention is All You Need" (2017)  
  [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)

