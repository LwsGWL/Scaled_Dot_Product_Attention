# Scaled_Dot_Product_Attention
This is the Scaled_Dot_Product_Attetion layer, implemented based on research from the "Attention is All You Need" paper and several open-source converters on GitHub. For detailed sources, please refer to the READM.md file.

Reference
