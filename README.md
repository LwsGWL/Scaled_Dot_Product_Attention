# Scaled_Dot_Product_Attention
This is the Scaled_Dot_Product_Attetion layer, implemented based on research from the "Attention is All You Need" paper and several open-source converters on GitHub. For detailed sources, please refer to the READM.md file.

## References
- "Attention is All You Need" (2017)  
  [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)

- 
