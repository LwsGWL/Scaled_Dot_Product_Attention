# Scaled_Dot_Product_Attention
This is the Scaled_Dot_Product_Attetion layer, implemented based on research from the "Attention is All You Need" paper and several open-source converters on GitHub. For detailed sources, please refer to the READM.md file.

## Reference
  ["Attention Is All You Need"][https://arxiv.org/abs/1706.03762]
